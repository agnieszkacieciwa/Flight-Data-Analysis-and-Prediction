import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

train_data = pd.read_csv(r"C:\Users\agnie\OneDrive\Pulpit\training.csv", sep='\t')
validation_data = pd.read_csv(r"C:\Users\agnie\OneDrive\Pulpit\validation.csv", sep='\t')

print(train_data.info())

print(validation_data.info())

print(train_data.describe())

print(validation_data.describe())

# Zamiana wartości w wybranych kolumnach na liczby 

columns_to_convert_1 = ['DepartureYear', 'DepartureMonth', 'DepartureDay', 
                      'FlightNumber', 'ActualFlightTime', 'ActualTotalFuel', 'ActualTOW', 
                      'FLownPassengers', 'BagsCount', 'FlightBagsWeight']
train_data[columns_to_convert_1] = train_data[columns_to_convert_1].apply(pd.to_numeric, errors='coerce')

train_data.head()


columns_to_convert_2 = ['DepartureYear', 'DepartureMonth', 'DepartureDay', 
                      'FlightNumber', 'ActualFlightTime', 'ActualTotalFuel', 
                      'FLownPassengers', 'BagsCount', 'FlightBagsWeight']
validation_data[columns_to_convert_2] = validation_data[columns_to_convert_2].apply(pd.to_numeric, errors='coerce')

validation_data.head()


# Rozkład zmiennych liczbowych

train_data.hist(bins=30, figsize=(15, 10))
plt.show()

# Korelacja między zmiennymi

correlation = train_data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Zastąpienie brakujących danych medianą 

train_data.fillna(train_data.median(), inplace=True)
validation_data.fillna(validation_data.median(), inplace=True)

train_data.head()

print(train_data.isnull().sum())
print(validation_data.isnull().sum())

# Funkcja do identyfikacji i usuwania wartości odstających

def remove_outliers(df, column_name):
    Q1 = df[column_name].quantile(0.25)
    Q3 = df[column_name].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers_index = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)].index
    df.drop(outliers_index, inplace=True)
    return df

# Usunięcie wartości odstających w danych treningowych

columns_to_check = ['ActualTOW', 'FLownPassengers', 'BagsCount', 'FlightBagsWeight']
for column in columns_to_check:
    train_data = remove_outliers(train_data, column)

# Normalizacja danych

scaler = StandardScaler()
columns_to_normalize = ['ActualTotalFuel', 'FLownPassengers', 'FlightBagsWeight']
train_data[columns_to_normalize] = scaler.fit_transform(train_data[columns_to_normalize])
validation_data[columns_to_normalize] = scaler.transform(validation_data[columns_to_normalize])


variables = ['ActualTotalFuel', 'FLownPassengers', 'FlightBagsWeight']

# Wykresy punktowe

for var in variables:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=var, y='ActualTOW', data=train_data)
    plt.title(f'Scatter Plot: {var} vs ActualTOW')
    plt.xlabel(var)
    plt.ylabel('ActualTOW')
    plt.grid(True)
    plt.show()


# Obliczanie korelacji między TOW a innymi zmiennymi

correlation = train_data[variables + ['ActualTOW']].corr()
print("Correlation Matrix:")
print(correlation)

X = train_data[['ActualTotalFuel', 'FLownPassengers', 'FlightBagsWeight']]
y = train_data['ActualTOW']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Model regresji liniowej

model = LinearRegression()
model.fit(X_train, y_train)
y_pred_linear = model.predict(X_test)
rmse_linear = mean_squared_error(y_test, y_pred_linear, squared=False)
print("RMSE dla regresji liniowej:", rmse_linear)

# Model Random Forest

rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)
print("Random Forest RMSE:", rmse_rf)

# Grid Search dla Random Forest

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)
best_rf_model = grid_search.best_estimator_
y_pred_best_rf = best_rf_model.predict(X_test)
rmse_best_rf = mean_squared_error(y_test, y_pred_best_rf, squared=False)
print("Best Random Forest RMSE:", rmse_best_rf)

# Zapisanie wyników predykcji do pliku CSV

validation_predictions = grid_search.predict(validation_data[['ActualTotalFuel', 'FLownPassengers', 'FlightBagsWeight']])

validation_data['PredictedTOW'] = validation_predictions
validation_data.to_csv("validation_predictions.csv", columns=['PredictedTOW'], index=False)
